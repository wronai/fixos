"""
FixOrchestrator ‚Äì g≈Ç√≥wna pƒôtla egzekucji napraw.
ZarzƒÖdza grafem problem√≥w, re-diagnozƒÖ po ka≈ºdym kroku i eskalacjƒÖ.
"""

from __future__ import annotations

import asyncio
import json
import time
import uuid
from typing import Optional

from ..config import FixOsConfig
from ..providers.llm import LLMClient, LLMError
from ..utils.anonymizer import anonymize
from .executor import CommandExecutor, ExecutionResult, DangerousCommandError, CommandTimeoutError
from .graph import Problem, ProblemGraph, ProblemSeverity


DIAGNOSE_PROMPT = """\
You are a Linux system repair assistant. Analyze the diagnostic data and identify problems.

System info: {os_info}
Known problems already in graph: {known_problems}
Diagnostic data (anonymized):
{diagnostic_data}

Return ONLY valid JSON (no markdown, no explanation outside JSON):
{{
  "new_problems": [
    {{
      "id": "p_<short_slug>",
      "description": "...",
      "severity": "critical|warning|info",
      "fix_commands": ["cmd1", "cmd2"],
      "related_to": []
    }}
  ],
  "explanation": "..."
}}
"""

EVALUATE_PROMPT = """\
You are a Linux system repair assistant. Evaluate the result of a fix attempt.

Problem that was fixed:
{problem}

Fix command executed: {command}
Return code: {returncode}
Stdout (anonymized): {stdout}
Stderr (anonymized): {stderr}

Based on the output, did the fix succeed? Are there any new problems discovered?

Return ONLY valid JSON:
{{
  "verdict": "resolved|failed|partial",
  "confidence": 0.0,
  "new_problems": [
    {{
      "id": "p_<short_slug>",
      "description": "...",
      "severity": "critical|warning|info",
      "fix_commands": ["cmd1"],
      "related_to": ["{problem_id}"]
    }}
  ],
  "explanation": "..."
}}
"""


class FixOrchestrator:
    """
    Orkiestrator napraw systemowych.
    
    Tryby:
    - hitl: ka≈ºda komenda wymaga potwierdzenia u≈ºytkownika
    - autonomous: automatyczne wykonanie gdy confidence > threshold
    """

    def __init__(
        self,
        config: FixOsConfig,
        executor: Optional[CommandExecutor] = None,
        auto_confirm_threshold: float = 0.90,
    ):
        self.config = config
        self.llm = LLMClient(config)
        self.executor = executor or CommandExecutor(
            default_timeout=120,
            require_confirmation=(config.agent_mode == "hitl"),
            dry_run=False,
        )
        self.graph = ProblemGraph()
        self.session_log: list[dict] = []
        self.auto_confirm_threshold = auto_confirm_threshold
        self._start_time = time.time()

    # ‚îÄ‚îÄ Public API ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def load_from_diagnostics(self, diagnostics: dict) -> list[Problem]:
        """Parsuje dane diagnostyczne przez LLM i buduje graf problem√≥w."""
        anon_str, _ = anonymize(str(diagnostics))
        os_info_raw = diagnostics.get("system", {}).get("os_release", "Linux")
        os_info, _ = anonymize(os_info_raw)

        known = [p.to_summary() for p in self.graph.nodes.values()]

        prompt = DIAGNOSE_PROMPT.format(
            os_info=os_info,
            known_problems=json.dumps(known, ensure_ascii=False),
            diagnostic_data=anon_str[:6000],
        )

        try:
            raw = self.llm.chat(
                [{"role": "user", "content": prompt}],
                max_tokens=2000,
                temperature=0.1,
            )
            data = self._parse_json(raw)
            problems = []
            for pd in data.get("new_problems", []):
                p = Problem(
                    id=pd.get("id") or f"p_{uuid.uuid4().hex[:6]}",
                    description=pd.get("description", "Nieznany problem"),
                    severity=pd.get("severity", "warning"),
                    fix_commands=pd.get("fix_commands", []),
                    caused_by=pd.get("related_to", []),
                )
                self.graph.add(p)
                problems.append(p)
            self._log("diagnose", {"found": len(problems), "explanation": data.get("explanation", "")})
            return problems
        except (LLMError, ValueError) as e:
            self._log("diagnose_error", {"error": str(e)})
            return []

    def load_from_dict(self, problems_data: list[dict]) -> list[Problem]:
        """≈Åaduje problemy bezpo≈õrednio z listy dict (bez LLM)."""
        problems = []
        for pd in problems_data:
            p = Problem(
                id=pd.get("id") or f"p_{uuid.uuid4().hex[:6]}",
                description=pd["description"],
                severity=pd.get("severity", "warning"),
                fix_commands=pd.get("fix_commands", []),
                caused_by=pd.get("caused_by", []),
            )
            self.graph.add(p)
            problems.append(p)
        return problems

    def run_sync(
        self,
        confirm_fn=None,
        progress_fn=None,
    ) -> dict:
        """
        Synchroniczna pƒôtla napraw (dla trybu HITL).
        
        Args:
            confirm_fn: callable(problem, command) -> bool ‚Äì pytaj u≈ºytkownika
            progress_fn: callable(problem, result) ‚Äì callback po ka≈ºdym kroku
        """
        if confirm_fn is None:
            confirm_fn = self._default_confirm
        if progress_fn is None:
            progress_fn = self._default_progress

        max_iterations = 50
        iteration = 0

        while not self.graph.all_done() and iteration < max_iterations:
            iteration += 1
            problem = self.graph.next_actionable()
            if problem is None:
                break

            problem.status = "in_progress"
            problem.attempts += 1

            # Wykonaj ka≈ºdƒÖ komendƒô fix
            last_result = None
            for cmd in problem.fix_commands:
                if not confirm_fn(problem, cmd):
                    problem.status = "pending"
                    self._log("skipped", {"problem_id": problem.id, "command": cmd})
                    break

                try:
                    result = self.executor.execute_sync(cmd)
                    last_result = result
                    self._log("executed", result.to_context())
                    progress_fn(problem, result)

                    if not result.success and result.executed:
                        # Komenda siƒô nie powiod≈Ça ‚Äì oce≈Ñ przez LLM
                        break
                except DangerousCommandError as e:
                    print(f"\n  ‚õî ZABLOKOWANO: {e}")
                    problem.status = "failed"
                    self._log("dangerous_blocked", {"command": cmd, "error": str(e)})
                    break
                except CommandTimeoutError as e:
                    print(f"\n  ‚è∞ TIMEOUT: {e}")
                    last_result = ExecutionResult(command=cmd, timed_out=True, executed=False)
                    break

            # Oce≈Ñ wynik przez LLM i wykryj nowe problemy
            if last_result is not None:
                new_problems = self._evaluate_and_rediagnose(problem, last_result)
                for np in new_problems:
                    np.caused_by.append(problem.id)
                    problem.may_cause.append(np.id)
                    self.graph.add(np)
                    print(f"\n  üîç Odkryto nowy problem: [{np.id}] {np.description}")

        return self._session_summary()

    async def run_async(self, confirm_fn=None, progress_fn=None) -> dict:
        """Asynchroniczna wersja run_sync."""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None, lambda: self.run_sync(confirm_fn, progress_fn)
        )

    # ‚îÄ‚îÄ Private helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _evaluate_and_rediagnose(
        self, problem: Problem, result: ExecutionResult
    ) -> list[Problem]:
        """Wysy≈Ça wynik do LLM, ocenia sukces i wykrywa nowe problemy."""
        anon_stdout, _ = anonymize(result.stdout)
        anon_stderr, _ = anonymize(result.stderr)

        prompt = EVALUATE_PROMPT.format(
            problem=json.dumps(problem.to_summary(), ensure_ascii=False),
            command=result.command,
            returncode=result.returncode,
            stdout=anon_stdout[:1500],
            stderr=anon_stderr[:500],
            problem_id=problem.id,
        )

        try:
            raw = self.llm.chat(
                [{"role": "user", "content": prompt}],
                max_tokens=1000,
                temperature=0.1,
            )
            data = self._parse_json(raw)
            verdict = data.get("verdict", "failed")
            confidence = float(data.get("confidence", 0.5))

            if verdict == "resolved" or (verdict == "partial" and confidence >= self.auto_confirm_threshold):
                problem.status = "resolved"
            elif problem.attempts >= problem.max_attempts:
                problem.status = "failed"
            else:
                problem.status = "pending"

            self._log("evaluate", {
                "problem_id": problem.id,
                "verdict": verdict,
                "confidence": confidence,
                "explanation": data.get("explanation", ""),
            })

            new_problems = []
            for pd in data.get("new_problems", []):
                p = Problem(
                    id=pd.get("id") or f"p_{uuid.uuid4().hex[:6]}",
                    description=pd.get("description", "Nieznany problem"),
                    severity=pd.get("severity", "warning"),
                    fix_commands=pd.get("fix_commands", []),
                    caused_by=pd.get("related_to", []),
                )
                new_problems.append(p)
            return new_problems

        except (LLMError, ValueError) as e:
            self._log("evaluate_error", {"error": str(e)})
            # Fallback: oceniaj po returncode
            if result.success:
                problem.status = "resolved"
            elif problem.attempts >= problem.max_attempts:
                problem.status = "failed"
            else:
                problem.status = "pending"
            return []

    def _parse_json(self, raw: str) -> dict:
        """Parsuje JSON z odpowiedzi LLM (usuwa markdown code fences)."""
        text = raw.strip()
        # Usu≈Ñ ```json ... ``` je≈õli obecne
        if text.startswith("```"):
            lines = text.splitlines()
            text = "\n".join(
                line for line in lines
                if not line.strip().startswith("```")
            )
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            # Spr√≥buj wyciƒÖgnƒÖƒá JSON z tekstu
            import re
            m = re.search(r"\{.*\}", text, re.DOTALL)
            if m:
                return json.loads(m.group())
            raise ValueError(f"Nie mo≈ºna sparsowaƒá JSON z odpowiedzi LLM: {raw[:200]}")

    def _log(self, event: str, data: dict) -> None:
        self.session_log.append({
            "event": event,
            "timestamp": time.time() - self._start_time,
            **data,
        })

    def _session_summary(self) -> dict:
        summary = self.graph.summary()
        summary["elapsed_seconds"] = int(time.time() - self._start_time)
        summary["log_entries"] = len(self.session_log)
        return summary

    @staticmethod
    def _default_confirm(problem: Problem, command: str) -> bool:
        severity_icon = {"critical": "üî¥", "warning": "üü°", "info": "üü¢"}.get(problem.severity, "‚ö™")
        print(f"\n  {severity_icon} [{problem.id}] {problem.description}")
        print(f"  ‚îå‚îÄ KOMENDA ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        print(f"  ‚îÇ  {command}")
        print(f"  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        ans = input("  Wykonaƒá? [Y/n/s(kip all)]: ").strip().lower()
        return ans in ("y", "yes", "")

    @staticmethod
    def _default_progress(problem: Problem, result: ExecutionResult) -> None:
        icon = "‚úÖ" if result.success else f"‚ùå (kod {result.returncode})"
        print(f"  {icon} {result.command}")
        if result.stdout and not result.stdout.startswith("(ju≈º wykonane"):
            preview = result.stdout[:300]
            print(f"  Output: {preview}")
